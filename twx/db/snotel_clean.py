'''
Utility functions for cleaning SNOTEL data.

SNOTEL data at NRCS is found in 2 main databases.  One is a historical database
of all SNOTEL station observations (some stations no longer in operation) up
until 9/2009 (as of 05/2012). Another tab-delimited file datasource contains
all observations for existing SNOTEL sites to the present day. Since the
tab-delimited datasource doesn't have observations from stations no longer
in existence, both datasources are used in combination. The functions here
bring the datasources together and output simple csv files that can then be
used as input to a binary form (i.e.--netCDF4). SNOTEL data is extremely
disorganized and a frustrating dataset with which to work.

Historical: ftp://ftp.wcc.nrcs.usda.gov/data/snow/snotel/snothist/
Observation File Format: ftp://ftp.wcc.nrcs.usda.gov/data/snow/snotel/snothist/README.txt

Current Tab-Delimited Files: ftp://ftp.wcc.nrcs.usda.gov/data/snow/snotel/cards

Snotel station data appears to have a daily time-of-observation = 12:00am, but
the way the data is organized seems different than the typical NOAA standards

My interpretation:
tmin,tmax,tavg list for a day are actually for the PREVIOUS day
swe (pill for a day is total swe on the ground at end of PREVIOUS day
prec (water year accum prcp) for a day is accum at end of PREVIOUS day
prcp for a day is total new prcp for the CURRENT day

'''

import os
from twx.utils import status_check
import numpy as np
import datetime
from datetime import timedelta

SNTL_DATAFILE_PREFIX = "snot"
A_DAY = timedelta(days=1)
MISSING = -9999.
VAL_STR = 10
VAL_END = 18
VAL_SHIFT = 8
MISSING_HIST = -999.9
SNTL_DSOURCE_HIST = "H"  # historical
SNTL_DSOURCE_CUR = "C"  # current
SNTL_DSOURCE_BOTH = "B"  # both
STATE_DIRS = {'AZ':'arizona', 'CA':'california', 'CO':'colorado',
              'ID':'idaho', 'MT':'montana', 'NV':'nevada', 'NM':'new_mexico', 'OR':'oregon', 'SD':'south_dakota',
              'UT':'utah', 'WA':'washington', 'WY':'wyoming', 'AK':'alaska'}


def snotel_find_no_metadata_stns(path_tab, fpath_stn_metadata):
    '''
    Find stations that have tab data, but don't have any metadata.
    This can happen if a new station is installed, there's an invalid file, etc.
    Print each station with this condition. The user needs to investigate them
    manually and add them manually to the stn metadata file if the station(s)
    should be taken into account and obs written using write_stn_obs.

    Parameters
    ----------
    path_tab : str
        Path to the root of the tab files. (contains tab files separated by
        state directories)
    fpath_stn_metadata : str
        File path of station metadata file generated by snotel_write_stn_metadata.
    '''

    f_in = open(fpath_stn_metadata)
    f_in.readline()

    stn_ids = []

    for line in f_in.readlines():

        vals = line.split(',')
        stn_ids.append(vals[0])

    stn_ids = np.array(stn_ids)

    for state in STATE_DIRS.values():

        files = np.array(os.listdir(os.path.join(path_tab, state)))

        files = files[np.logical_or(np.core.defchararray.find(files, ".tab") != -1,
                                    np.core.defchararray.find(files, ".txt") != -1)]

        nodata_stns = []
        for afile in files:

            stn_id = afile.split("_")[0]

            if stn_id not in stn_ids:

                stn_tab = file(os.path.join(path_tab, state, afile))
                # possible headings: ['date', 'pill', 'prec', 'tmax', 'tmin', 'tavg', 'prcp']
                headings = stn_tab.readline().split('\t')
                headings = [x.strip() for x in headings]

                try:
                    stn_name = headings[0].split('-')[0][2:]
                except IndexError:
                    stn_name = "None"

                nodata_stns.append((stn_id, stn_name))

        nodata_stns = np.unique(nodata_stns)

        for stn in nodata_stns:
            print ": ".join([state, str(stn)])


def snotel_write_stn_metadata(path_hist, fpath_highres, path_out):
    '''
    Consolidate all SNOTEL station metadata into a single simple csv.
    Data includes station id, name, state, datasource (historical,
    current tab, both), lat, lon, and elev.

    Parameters
    ----------
    path_hist : str
        Path to the root of the historical data.
    fpath_highres : str
        File path to precise SNOTEL location information. Precise SNOTEL
        location information is restricted and must be obtained from NRCS.
    path_out: str
        Path to which the snotel_stns.csv station metadata file should be written
    '''

    stns = _build_stn_metadata(path_hist, fpath_highres)

    # Write out station metadata to simple csv file
    fout = open("".join([path_out, "snotel_stns.csv"]), 'w')
    fout.write(",".join(["STN_ID", "NAME", "STATE", "DSOURCE", "LAT", "LON",
                         "ELEV" + "\n"]))
    for stn_id in stns.keys():
        stn_id, name, st, dsrc, lat, lon, elev = stns[stn_id]
        fout.write(",".join([stn_id, name, str(st), dsrc, str(lat), str(lon),
                             str(elev) + "\n"]))
    fout.close()


def snotel_write_stn_obs(path_hist, path_tab, fpath_stn_metadata, path_out):
    '''
    Combine historical datasource and current tab datasource station observations
    and output them into a single simple csv for each station. Priority of
    overlapping daily observations from the different datasources is:
    tab all file --> tab single year file --> historical data. This function will
    take a little while to run as it first loads all historical data into memory
    and then outputs each station one-by-one.
    The headers for each csv file are:
    "YMD","TMIN","TMAX","PRCP","TAVG","PREC","PILL"

    Parameters
    ----------
    path_hist : str
        Path to the root of the historical data.
    path_tab : str
        Path to the root of the tab files. (contains tab files separated by
        state directories)
    fpath_stn_metadata : str
        File path of station metadata file generated by snotel_write_stn_metadata.
    path_out: str
        Path to which the station observation files should be written
    '''

    f_in = open(fpath_stn_metadata)
    f_in.readline()

    stn_ids = []
    states = []

    for line in f_in.readlines():

        vals = line.split(',')
        stn_ids.append(vals[0])
        states.append(vals[2])

    # Load all historical data into memory
    # This will take a ~6 minutes
    obs_hist = _load_obs_hist(path_hist)

    print "Writing cleaned station observations..."
    statchk = status_check(len(stn_ids), 10)

    for x in np.arange(len(stn_ids)):

        # Load current tab observations for this station
        obs_cur = _load_cur_obs(stn_ids[x], states[x], path_tab)

        # Combine historic and current tab datasources if necessary
        if obs_hist.has_key(stn_ids[x]) and len(obs_cur) > 0:
            obs_final = _combine_hist_cur_obs(obs_cur, obs_hist[stn_ids[x]])
        elif obs_hist.has_key(stn_ids[x]) and len(obs_cur) == 0:
            obs_final = obs_hist[stn_ids[x]]
        elif not obs_hist.has_key(stn_ids[x]) and len(obs_cur) > 0:
            obs_final = obs_cur
        else:
            # no obs for this station
            obs_final = {}

        if len(obs_final) > 0:

            fout = open(os.path.join(path_out, "%s.csv" % (stn_ids[x],)), "w")
            fout.write(",".join(["YMD", "TMIN", "TMAX", "PRCP", "TAVG", "PREC", "".join(["PILL\n"])]))

            for ymd in np.sort(obs_final.keys()):

                vals = obs_final[ymd]
                vals = [str(x) for x in vals]

                aline = ",".join(vals)
                aline = "".join([aline, "\n"])

                fout.write(aline)

            fout.close()

        else:

            print "No obs for stn: " + stn_ids[x]

        statchk.increment()


def _combine_hist_cur_obs(obs_cur, obs_hist):

    obs_final = {}
    obs_final.update(obs_cur)

    ymd_cur = obs_cur.keys()
    ymd_cur = np.array(ymd_cur)

    ymd_hist = obs_hist.keys()
    ymd_hist = np.array(ymd_hist)

    all_mask = np.in1d(ymd_hist, ymd_cur, assume_unique=True)

    ymd_hist_only = ymd_hist[np.logical_not(all_mask)]

    for ymd in ymd_hist_only:
        obs_final[ymd] = obs_hist[ymd]

    return obs_final


def _load_cur_obs(stn_id, state, path_tab):

    path_state = os.path.join(path_tab, STATE_DIRS[state])

    files = np.array(os.listdir(path_state))

    files_stn = files[np.core.defchararray.find(files, stn_id) != -1]

    all_file_mask = np.core.defchararray.find(files_stn, "all") != -1

    all_file = files_stn[all_file_mask]
    if all_file.size > 1:
        raise Exception("More than 1 all file for stn: " + stn_id)

    all_file = all_file[0] if all_file.size == 1 else None

    obs_all = _parse_stn_tab(os.path.join(path_state, all_file)) if all_file is not None else {}

    obs_yrs = {}
    for afile in files_stn[np.logical_not(all_file_mask)]:
        obs_yrs.update(_parse_stn_tab(os.path.join(path_state, afile)))

    obs_final = {}
    if len(obs_all) > 0 and len(obs_yrs) > 0:

        obs_final.update(obs_all)

        ymd_all = obs_all.keys()
        ymd_all = np.array(ymd_all)

        ymd_yrs = obs_yrs.keys()
        ymd_yrs = np.array(ymd_yrs)

        all_mask = np.in1d(ymd_yrs, ymd_all, assume_unique=True)

        ymd_yrs_only = ymd_yrs[np.logical_not(all_mask)]

        for ymd in ymd_yrs_only:
            obs_final[ymd] = obs_yrs[ymd]
    elif len(obs_all) > 0 and len(obs_yrs) == 0:
        obs_final = obs_all
    elif len(obs_all) == 0 and len(obs_yrs) > 0:
        obs_final = obs_yrs

    return obs_final


def _build_stn_metadata(path_hist, fpath_highres):

    stns_hist = _parse_hist_stns(path_hist)
    stns_hs = _parse_highres_stns(fpath_highres)
    stns_hist = _update_stn_locs(stns_hist, stns_hs)

    stn_ids = stns_hist.keys()
    stn_ids.extend(stns_hs.keys())

    stn_ids = np.unique(np.array(stn_ids))

    stns_final = {}
    for stn_id in stn_ids:

        if stns_hist.has_key(stn_id) and stns_hs.has_key(stn_id):
            dsrc = SNTL_DSOURCE_BOTH
            name, lat, lon, elev, st = stns_hist[stn_id]
        elif stns_hist.has_key(stn_id) and not stns_hs.has_key(stn_id):
            dsrc = SNTL_DSOURCE_HIST
            name, lat, lon, elev, st = stns_hist[stn_id]
        elif not stns_hist.has_key(stn_id) and stns_hs.has_key(stn_id):
            dsrc = SNTL_DSOURCE_CUR
            name, lat, lon, elev, st = stns_hs[stn_id]
        else:
            raise Exception("STN ID not found in historical or current database: " +
                             str(stn_id))

        stns_final[stn_id] = [stn_id, name, st, dsrc, lat, lon, elev]

    return stns_final


def _update_stn_locs(stns, stns_highres):

    stns_new = {}
    all_hs = stns_highres.items()

    for stn_id in stns.keys():

        name = stns[stn_id][0]
        state = stns[stn_id][4]
        elev = stns[stn_id][3]

        if stns_highres.has_key(stn_id):

            namehs, lat, lon, elev, st = stns_highres[stn_id]

            # Make sure there is not a typo with the historical station id
            if namehs != name:
                print "Station name in high res locations does not match historical name: " + namehs + "|" + name + "|" + stn_id
                hgh_fnd = False

                for hs_id, hghres in all_hs:

                    if hghres[0] == name:

                        namehs, lat, lon, elev, st = hghres
                        hgh_fnd = True
                        stn_id = hs_id
                        break
                if not hgh_fnd:
                    raise Exception("High res location found, but no station name match for " + stn_id)

        else:

            print stn_id + " in " + state + " did not have high res location info.| " + str(elev)
            name, lat, lon, elev, st = stns[stn_id]

        stns_new[stn_id.lower()] = [name, lat, lon, elev, st]

    return stns_new


def _parse_hist_stns(path_hist):

    file_names = os.listdir(path_hist)

    stns = {}

    for file_name in file_names:

        # open list[state].txt files that have station information
        if "list" in file_name:

            afile = open(os.path.join(path_hist, file_name))

            # in listfiles, the state abbreviation is the last 2 characters
            # before the .txt extension
            state = file_name[-6:-4].upper()

            while True:
                line = afile.readline()
                if '---' in line:
                    break
            for line in afile.readlines():
                stn_id = line[26:34].strip().lower()
                # Lat and Lon are given in degrees, minutes
                lat = float(line[35:37]) + (float(line[37:39]) / 60.0)
                lon = -1 * (float(line[40:43]) + (float(line[43:45]) / 60.0))
                # convert from feet to meters
                try:
                    elev = float(line[46:51]) * 0.3048
                except ValueError:
                    elev = 0

                name = line[52:].strip()

                stns[stn_id.lower()] = [name, lat, lon, elev, state]

    return stns


def _parse_highres_stns(fpath):

    a_file = file(fpath)
    a_file.readline()

    locs_highres = {}
    for line in a_file.readlines():
        vals = line.split(",")
        st = vals[0].strip().upper()
        name = vals[1].strip()
        id = vals[2].strip().lower()

        if id == '':
            continue

        lat = float(vals[5].strip())
        lon = float(vals[6].strip())
        elev = float(vals[8].strip()) * 0.3048  # convert from feet to meters

        locs_highres[id] = [name, lat, lon, elev, st]
    return locs_highres


def _parse_stn_tab(fpath):

    stn_id = fpath.split(os.sep)[-1].split("_")[0]
    stn_tab = file(fpath)
    # possible headings: ['date', 'pill', 'prec', 'tmax', 'tmin', 'tavg', 'prcp']
    headings = stn_tab.readline().split('\t')

    headings = [x.strip() for x in headings]

    try:
        headings[0] = headings[0].split('-')[1]
    except IndexError:
        print "Invalid file: " + fpath
        return {}

    obs = {}

    date = None

    for line in stn_tab.readlines():

        vals = line.split("\t")
        vals = [x.strip() for x in vals]

        year = int(vals[0][4:])
        # correct for 2 digit year
        if year > 65:  # data starts at water year 1966
            year = year + 1900
        else:
            year = year + 2000

        month = int(vals[0][0:2])
        day = int(vals[0][2:4])

        date = datetime.date(year, month, day)
        prev_date = date - A_DAY

        ymd = long(date.strftime("%Y%m%d"))
        ymd_prev = long(prev_date.strftime("%Y%m%d"))

        if not obs.has_key(ymd):
            obs[ymd] = [ymd, MISSING, MISSING, MISSING,  # TMIN,TMAX,PRCP
                            MISSING, MISSING, MISSING]  # TAVG,PREC,PILL

        if not obs.has_key(ymd_prev):
            obs[ymd_prev] = [ymd_prev, MISSING, MISSING, MISSING,  # TMIN,TMAX,PRCP
                                      MISSING, MISSING, MISSING]  # TAVG,PREC,PILL

        for x in np.arange(len(vals)):

            if headings[x] == 'date':
                continue
            elif headings[x] == 'tmin':
                if len(vals[x]) > 0: obs[ymd_prev][1] = float(vals[x])  # tmin
            elif headings[x] == 'tmax':
                if len(vals[x]) > 0: obs[ymd_prev][2] = float(vals[x])  # tmax
            elif headings[x] == 'prcp':
                if len(vals[x]) > 0: obs[ymd][3] = float(vals[x]) * 2.54  # prcp convert inches to cm
            elif headings[x] == 'tavg':
                if len(vals[x]) > 0: obs[ymd_prev][4] = float(vals[x])  # tavg
            elif headings[x] == 'prec':
                if len(vals[x]) > 0: obs[ymd_prev][5] = float(vals[x]) * 2.54  # prec convert inches to cm
            elif headings[x] == 'pill':
                if len(vals[x]) > 0: obs[ymd_prev][6] = float(vals[x]) * 2.54  # swe convert inches to cm
    return obs


def _load_obs_hist(path_hist):

    print "SNOTEL: Loading historical data"
    fileNames = os.listdir(path_hist)
    obs_dict = {}

    for fileName in fileNames:

        if SNTL_DATAFILE_PREFIX in fileName:

            filePath = os.path.join(path_hist, fileName)
            afile = open(filePath)

            while True:

                line = afile.readline()

                if not line:
                    break

                if 'Station' in line:

                    stn_id, name = line.split(",")
                    stn_id = stn_id.split(":")[-1].strip().lower()

                    if not obs_dict.has_key(stn_id):
                        obs_dict[stn_id] = {}

                    # next 4 lines are not meaningful
                    for i in range(4):
                        afile.readline()

                    # next line has column headings - these matter!
                    line = afile.readline()
                    headings = line.split()[1:]  # skip DATE heading
                    nhead = len(headings)
                    rnghead = np.arange(nhead)

                    # now we have a bunch of data lines
                    while True:
                        line = afile.readline()
                        if line[:3] == '---' or not line:
                            break  # next station
                        year = int(line[0:2])
                        # correct for 2 digit year
                        if year > 65:  # data starts at water year 1966
                            year = year + 1900
                        else:
                            year = year + 2000

                        month = int(line[2:4])
                        day = int(line[4:6])

                        date = datetime.date(year, month, day)
                        prev_date = date - A_DAY
                        ymd = long(date.strftime("%Y%m%d"))
                        ymd_prev = long(prev_date.strftime("%Y%m%d"))

                        # iterate through the values left in the line and
                        # use the headings above to interpret them
                        swe = MISSING
                        tmax = MISSING
                        tmin = MISSING
                        tavg = MISSING
                        prec = MISSING
                        prcp = MISSING

                        for x in rnghead:

                            start = x * VAL_SHIFT + VAL_STR
                            end = x * VAL_SHIFT + VAL_END

                            v = line[start:end].strip()
                            if len(v) > 0 and float(v) != MISSING_HIST:

                                h = headings[x]

                                if h == 'pill':
                                    swe = float(v) * 2.54  # convert inches to cm
                                elif h == 'tmax':
                                    tmax = float(v)
                                elif h == 'tmin':
                                    tmin = float(v)
                                elif h == 'tavg':
                                    tavg = float(v)
                                elif h == 'prec':
                                    prec = float(v) * 2.54  # convert inches to cm
                                elif h == 'prcp':
                                    prcp = float(v) * 2.54  # convert inches to cm

                        if not obs_dict[stn_id].has_key(ymd):
                            obs_dict[stn_id][ymd] = [ymd, MISSING, MISSING, MISSING,  # TMIN,TMAX,PRCP
                                            MISSING, MISSING, MISSING]  # TAVG,PREC,PILL

                        if not obs_dict[stn_id].has_key(ymd_prev):
                            obs_dict[stn_id][ymd_prev] = [ymd_prev, MISSING, MISSING, MISSING,  # TMIN,TMAX,PRCP
                                                  MISSING, MISSING, MISSING]  # TAVG,PREC,PILL

                        obs_dict[stn_id][ymd_prev][1] = tmin
                        obs_dict[stn_id][ymd_prev][2] = tmax
                        obs_dict[stn_id][ymd][3] = prcp
                        obs_dict[stn_id][ymd_prev][4] = tavg
                        obs_dict[stn_id][ymd_prev][5] = prec
                        obs_dict[stn_id][ymd_prev][6] = swe

    print "SNOTEL: Done loading historical data"

    return obs_dict
